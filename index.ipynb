{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: 3D Gaussian Splatting\n",
    "\n",
    "Author: [Mukai (Tom Notch) Yu](https://tomnotch.com)\n",
    "\n",
    "Email: [mukaiy@andrew.cmu.edu](mailto:mukaiy@andrew.cmu.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm training everything with A100-80G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 3D Gaussian Splatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5 Perform Splatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![q1.1.5](Q1/output/q1.1.5_render.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Perform Forward Pass and Compute Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![progress](Q1/output/q1_training_progress.gif)\n",
    "\n",
    "\"wriggling gaussians\"\n",
    "\n",
    "![final](Q1/output/q1_training_final_renders.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation --- Mean PSNR: 28.489\n",
    "\n",
    "Evaluation --- Mean SSIM: 0.930"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "parameters = [\n",
    "    {\"params\": [gaussians.pre_act_opacities], \"lr\": 0.05, \"name\": \"opacities\"},\n",
    "    {\"params\": [gaussians.pre_act_scales], \"lr\": 0.01, \"name\": \"scales\"},\n",
    "    {\"params\": [gaussians.colors], \"lr\": 0.05, \"name\": \"colors\"},\n",
    "    {\"params\": [gaussians.means], \"lr\": 0.001, \"name\": \"means\"},\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1k iterations, took 19min 29s final loss = 0.008, reached after 100 iterations so shouldn't take that many iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Rendering Using Spherical Harmonics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Q1/output/q1.1.5_render.gif\" alt=\"No view-dependent\" width=\"45%\"/>\n",
    "  &nbsp; &nbsp; &nbsp; &nbsp; <!-- This is for adding space between the images -->\n",
    "  <img src=\"Q1/output/q1.3.1_render.gif\" alt=\"View-dependent\" width=\"45%\"/>\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  <em>Not view-dependent</em>\n",
    "  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\n",
    "  <em>View-dependent</em>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Q1/output/q1_render_no_view_dependent/015.png\" alt=\"No view-dependent\" width=\"45%\"/>\n",
    "  &nbsp; &nbsp; &nbsp; &nbsp; <!-- This is for adding space between the images -->\n",
    "  <img src=\"Q1/output/q1_render/015.png\" alt=\"View-dependent\" width=\"45%\"/>\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  <em>Not view-dependent</em>\n",
    "  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\n",
    "  <em>View-dependent</em>\n",
    "</p>\n",
    "\n",
    "I think it's quite evident, the velvet on the chair has complicated BRDF, resulting in pseudo-shadow\n",
    "\n",
    "Besides, the golden imprints appears more specular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Q1/output/q1_render_no_view_dependent/012.png\" alt=\"No view-dependent\" width=\"45%\"/>\n",
    "  &nbsp; &nbsp; &nbsp; &nbsp; <!-- This is for adding space between the images -->\n",
    "  <img src=\"Q1/output/q1_render/012.png\" alt=\"View-dependent\" width=\"45%\"/>\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  <em>Not view-dependent</em>\n",
    "  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\n",
    "  <em>View-dependent</em>\n",
    "</p>\n",
    "\n",
    "Also pseudo-shadow at the edge of the chair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Diffusion-guided Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 SDS Loss + Image Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Q2/output/image/a_hamburger_no_guidance/output.png\" alt=\"Without guidance\" width=\"45%\"/>\n",
    "  &nbsp; &nbsp; &nbsp; &nbsp; <!-- This is for adding space between the images -->\n",
    "  <img src=\"Q2/output/image/a_hamburger/output.png\" alt=\"With guidance\" width=\"45%\"/>\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  <em>Without guidance (2000 iterations) a hamburger</em>\n",
    "  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\n",
    "  <em>With guidance (2000 iterations) a hamburger</em>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Q2/output/image/a_standing_corgi_dog_no_guidance/output.png\" alt=\"Without guidance\" width=\"45%\"/>\n",
    "  &nbsp; &nbsp; &nbsp; &nbsp; <!-- This is for adding space between the images -->\n",
    "  <img src=\"Q2/output/image/a_standing_corgi_dog/output.png\" alt=\"With guidance\" width=\"45%\"/>\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  <em>Without guidance (2000 iterations) a standing corgi dog</em>\n",
    "  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\n",
    "  <em>With guidance (2000 iterations) a standing corgi dog</em>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Q2/output/image/diffusion_no_guidance/output.png\" alt=\"Without guidance\" width=\"45%\"/>\n",
    "  &nbsp; &nbsp; &nbsp; &nbsp; <!-- This is for adding space between the images -->\n",
    "  <img src=\"Q2/output/image/diffusion/output.png\" alt=\"With guidance\" width=\"45%\"/>\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  <em>Without guidance (2000 iterations) diffusion</em>\n",
    "  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\n",
    "  <em>With guidance (2000 iterations) diffusion</em>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Q2/output/image/a_snake_poking_its_head_out_of_a_water_bottle_like_aquarius,_while_its_body_swirls_at_the_bottom_of_the_bottle_no_guidance/output.png\" alt=\"Without guidance\" width=\"45%\"/>\n",
    "  &nbsp; &nbsp; &nbsp; &nbsp; <!-- This is for adding space between the images -->\n",
    "  <img src=\"Q2/output/image/a_snake_poking_its_head_out_of_a_water_bottle_like_aquarius,_while_its_body_swirls_at_the_bottom_of_the_bottle/output.png\" alt=\"With guidance\" width=\"45%\"/>\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "  <em>Without guidance (2000 iterations)</em>\n",
    "  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\n",
    "  <em>With guidance (2000 iterations) (this is bull shit)</em>\n",
    "</p>\n",
    "\n",
    " a snake poking its head out of a water bottle like aquarius, while its body swirls at the bottom of the bottle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Texture Map Optimization for Mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hamburger](Q2/output/mesh/a_hamburger/final_mesh.gif)\n",
    "\n",
    "A hamburger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![shire](Q2/output/mesh/a_village_like_Shire_in_the_lord_of_the_rings/final_mesh.gif)\n",
    "\n",
    "A village like Shire in the lord of the rings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think random viewpoint and light source would worsen result, because we cannot render or \"imprint\" the image onto the mesh from a particular viewpoint, but rather from an \"averaged\" viewpoint. Eventually, we are only \"imprinting\" the dominant color,\n",
    "\n",
    "The geometry is fixed, and we do not encode viewpoint information in the text prompt when we generate latents,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 NeRF Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda_{entropy} = 10^{-2}$, $\\lambda_{orient} = 10^{-3}$, latent iteration: $20\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![corgi](Q2/output/nerf/a_standing_corgi_dog_no_view_dependent/gif/rgb_ep_99.gif)\n",
    "\n",
    "A standing corgi dog\n",
    "\n",
    "Final loss after 100 epochs, 2291.58 seconds: 0.445"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hamburger](Q2/output/nerf/a_hamburger_no_view_dependent/gif/rgb_ep_99.gif)\n",
    "\n",
    "A hamburger\n",
    "\n",
    "Final loss after 100 epochs, 2438.83 seconds: 0.315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![snake](Q2/output/nerf/a_Slytherin_snake_no_view_dependent/gif/rgb_ep_99.gif)\n",
    "\n",
    "A Slytherin snake\n",
    "\n",
    "Final loss after 100 epochs, 2288.24 seconds: 0.648\n",
    "\n",
    "(pretty sure this is not a snake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 View-dependent text embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![corgi](Q2/output/nerf/a_standing_corgi_dog/gif/rgb_ep_99.gif)\n",
    "\n",
    "A standing corgi dog\n",
    "\n",
    "Final loss after 100 epochs, 2293.36 seconds: 0.563"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hamburger](Q2/output/nerf/a_hamburger/gif/rgb_ep_99.gif)\n",
    "\n",
    "A hamburger\n",
    "\n",
    "Final loss after 100 epochs, 2300.36 seconds: 0.769"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![snake](Q2/output/nerf/a_Slytherin_snake/gif/rgb_ep_99.gif)\n",
    "\n",
    "A Slytherin snake\n",
    "\n",
    "Final loss after 100 epochs, 2300.79 seconds: 0.439"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The view-dependent text encoding fixed the no-head problem with snake, this is because having multiple views can reveal the obstructed geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, I think this is still a hacky solution, because simply appending the view text does not guarantee that the diffusion model understands or focuses on different views in either CV or NLP sense. This is evident in the corgi dog experiments, we had better results even without view-dependent text embedding."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
